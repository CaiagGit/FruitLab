{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c23a284",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==============================================================================================================================================================\n",
    "# INSTANCIATION DU SERVEUR AMAZON ELASTIC MAPREDUCE (EMR)\n",
    "#==============================================================================================================================================================\n",
    "\n",
    "# Amazon EMR : 6.10.0\n",
    "# Hadoop 3.3.3\n",
    "# Spark 3.3.1\n",
    "# TensorFlow 2.11.0\n",
    "# Ganglia 3.7.2\n",
    "# JupyterHub 1.5.0\n",
    "# Python 3.6\n",
    "\n",
    "#EEEEEEEEEEEEEEEEEEE MMMMMMMM           MMMMMMMM RRRRRRRRRRRRRRR    \n",
    "#::::::::::::::::::E M:::::::M         M:::::::M R::::::::::::::R   \n",
    "#E:::::EEEEEEEEE:::E M::::::::M       M::::::::M R:::::RRRRRR:::::R \n",
    "  #::::E       EEEEE M:::::::::M     M:::::::::M RR::::R      R::::R\n",
    "  #::::E             M::::::M:::M   M:::M::::::M   R:::R      R::::R\n",
    "  #:::::EEEEEEEEEE   M:::::M M:::M M:::M M:::::M   R:::RRRRRR:::::R \n",
    "  #::::::::::::::E   M:::::M  M:::M:::M  M:::::M   R:::::::::::RR   \n",
    "  #:::::EEEEEEEEEE   M:::::M   M:::::M   M:::::M   R:::RRRRRR::::R  \n",
    "  #::::E             M:::::M    M:::M    M:::::M   R:::R      R::::R\n",
    "  #::::E       EEEEE M:::::M     MMM     M:::::M   R:::R      R::::R\n",
    "#E:::::EEEEEEEE::::E M:::::M             M:::::M   R:::R      R::::R\n",
    "#::::::::::::::::::E M:::::M             M:::::M RR::::R      R::::R\n",
    "#EEEEEEEEEEEEEEEEEEE MMMMMMM             MMMMMMM RRRRRRR      RRRRRR\n",
    " \n",
    "# sudo yum update -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d66fe910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>12</td><td>application_1678693160646_0013</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-2-80.eu-west-3.compute.internal:20888/proxy/application_1678693160646_0013/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-9-154.eu-west-3.compute.internal:8042/node/containerlogs/container_1678693160646_0013_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=yarn appName=livy-session-12>"
     ]
    }
   ],
   "source": [
    "#===============================================================================================================\n",
    "# 1. CRÉATION D'UNE SESSION SPARK \n",
    "#===============================================================================================================\n",
    "\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e94331c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>13</td><td>application_1678693160646_0014</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-2-80.eu-west-3.compute.internal:20888/proxy/application_1678693160646_0014/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-9-154.eu-west-3.compute.internal:8042/node/containerlogs/container_1678693160646_0014_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.pyspark.python': 'python3', 'spark.pyspark.virtualenv.enabled': 'true', 'spark.pyspark.virtualenv.type': 'native', 'spark.pyspark.virtualenv.bin.path': '/usr/bin/virtualenv'}, 'proxyUser': 'jovyan', 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>13</td><td>application_1678693160646_0014</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-2-80.eu-west-3.compute.internal:20888/proxy/application_1678693160646_0014/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-9-154.eu-west-3.compute.internal:8042/node/containerlogs/container_1678693160646_0014_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\"conf\": {\n",
    "      \"spark.pyspark.python\": \"python3\",\n",
    "      \"spark.pyspark.virtualenv.enabled\": \"true\",\n",
    "      \"spark.pyspark.virtualenv.type\": \"native\",\n",
    "      \"spark.pyspark.virtualenv.bin.path\": \"/usr/bin/virtualenv\"\n",
    "   }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84b445d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sparkdl\n",
      "  Downloading sparkdl-0.2.2-py3-none-any.whl (99 kB)\n",
      "Installing collected packages: sparkdl\n",
      "Successfully installed sparkdl-0.2.2\n",
      "\n",
      "Collecting pillow\n",
      "  Downloading Pillow-9.4.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "Installing collected packages: pillow\n",
      "Successfully installed pillow-9.4.0\n",
      "\n",
      "Collecting pyspark\n",
      "  Downloading pyspark-3.3.2.tar.gz (281.4 MB)\n",
      "Collecting py4j==0.10.9.5\n",
      "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py): started\n",
      "  Building wheel for pyspark (setup.py): finished with status 'done'\n",
      "  Created wheel for pyspark: filename=pyspark-3.3.2-py2.py3-none-any.whl size=281826065 sha256=0ccca0c4fca9565922b8f7c18ab7e7ed09eec02ca0226f9f4ede5b74a4f9e3c7\n",
      "  Stored in directory: /mnt/tmp/pip-ephem-wheel-cache-pfmw40ms/wheels/07/fb/67/b9f2c0242d156eaa136b45ae4fd99d3e7c0ecc2acfd26f47b9\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.5 pyspark-3.3.2\n",
      "\n",
      "Collecting pandas\n",
      "  Downloading pandas-1.3.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
      "Collecting python-dateutil>=2.7.3\n",
      "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "Requirement already satisfied: numpy>=1.17.3; platform_machine != \"aarch64\" and platform_machine != \"arm64\" and python_version < \"3.10\" in /usr/local/lib64/python3.7/site-packages (from pandas) (1.20.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas) (1.13.0)\n",
      "Installing collected packages: python-dateutil, pandas\n",
      "Successfully installed pandas-1.3.5 python-dateutil-2.8.2\n",
      "\n",
      "Requirement already satisfied: tensorflow in /usr/local/lib64/python3.7/site-packages (2.11.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib64/python3.7/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib64/python3.7/site-packages (from tensorflow) (1.6.3)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib64/python3.7/site-packages (from tensorflow) (2.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib64/python3.7/site-packages (from tensorflow) (1.12.1)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1; platform_machine != \"arm64\" or platform_system != \"Darwin\"\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.31.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib64/python3.7/site-packages (from tensorflow) (2.11.0)\n",
      "Requirement already satisfied: setuptools in ./tmp/1678698566401-0/lib/python3.7/site-packages (from tensorflow) (28.8.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib64/python3.7/site-packages (from tensorflow) (22.12.6)\n",
      "Collecting protobuf<3.20,>=3.9.2\n",
      "  Downloading protobuf-3.19.6-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "Requirement already satisfied: numpy>=1.20 in /usr/local/lib64/python3.7/site-packages (from tensorflow) (1.20.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib64/python3.7/site-packages (from tensorflow) (2.10.0)\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-15.0.6.1-py2.py3-none-manylinux2010_x86_64.whl (21.5 MB)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/site-packages (from tensorflow) (1.13.0)\n",
      "Collecting keras<2.12,>=2.11.0\n",
      "  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib64/python3.7/site-packages (from tensorflow) (22.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib64/python3.7/site-packages (from tensorflow) (1.29.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib64/python3.7/site-packages (from tensorflow) (4.4.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/site-packages (from tensorflow) (2.3.2)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.7/site-packages (from tensorflow) (1.3.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in ./tmp/1678698566401-0/lib/python3.7/site-packages (from astunparse>=1.6.0->tensorflow) (0.29.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib64/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.4.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib64/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.28.1)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib64/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (1.23.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib64/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (1.8.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib64/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib64/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (1.26.13)\n",
      "Collecting charset-normalizer<3,>=2\n",
      "  Downloading charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib64/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib64/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2022.12.7)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib64/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.5\" in /usr/local/lib64/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (4.6)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib64/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (4.1.1)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib64/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib64/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib64/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib64/python3.7/site-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (3.4.0)\n",
      "Installing collected packages: gast, tensorflow-io-gcs-filesystem, protobuf, libclang, keras, tensorboard-data-server, charset-normalizer\n",
      "Successfully installed charset-normalizer-2.1.1 gast-0.4.0 keras-2.11.0 libclang-15.0.6.1 protobuf-3.19.6 tensorboard-data-server-0.6.1 tensorflow-io-gcs-filesystem-0.31.0\n",
      "\n",
      "Collecting pyarrow\n",
      "  Downloading pyarrow-11.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.1 MB)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib64/python3.7/site-packages (from pyarrow) (1.20.0)\n",
      "Installing collected packages: pyarrow\n",
      "Successfully installed pyarrow-11.0.0\n",
      "\n",
      "WARNING: The directory '/home/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\n",
      "\n",
      "WARNING: The directory '/home/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\n",
      "\n",
      "WARNING: The directory '/home/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\n",
      "ERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "python37-sagemaker-pyspark 1.4.2 requires pyspark==2.4.0, but you'll have pyspark 3.3.2 which is incompatible.\n",
      "\n",
      "WARNING: The directory '/home/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\n",
      "\n",
      "WARNING: The directory '/home/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\n",
      "ERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "tensorboard 2.11.0 requires setuptools>=41.0.0, but you'll have setuptools 28.8.0 which is incompatible.\n",
      "\n",
      "WARNING: The directory '/home/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag."
     ]
    }
   ],
   "source": [
    "#===============================================================================================================\n",
    "# 2. MODULES DÉPENDANCES ET BIBLIOTHÈQUES (JupyterHub)\n",
    "#===============================================================================================================\n",
    "\n",
    "packages = [\"sparkdl\", \"pillow\", \"pyspark\", \"pandas\", \"tensorflow\", \"pyarrow\"]\n",
    "for package in packages:\n",
    "    sc.install_pypi_package(package)\n",
    "\n",
    "#sc.install_pypi_package(\"pyarrow\")\n",
    "#sc.uninstall_package(\"pyarrow\")    \n",
    "#sc.list_packages()\n",
    "\n",
    "from pyspark.sql.functions import element_at, split, pandas_udf, PandasUDFType, col\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.linalg import VectorUDT, Vectors\n",
    "from PIL import Image\n",
    "\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import io\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "319c276f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- path: string (nullable = true)\n",
      " |-- modificationTime: timestamp (nullable = true)\n",
      " |-- length: long (nullable = true)\n",
      " |-- content: binary (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      "\n",
      "None\n",
      "+-------------------------------------------------+-----------+\n",
      "|path                                             |label      |\n",
      "+-------------------------------------------------+-----------+\n",
      "|s3://fruitlabbucket/images/apple_red_3/r1_67.jpg |apple_red_3|\n",
      "|s3://fruitlabbucket/images/apple_red_3/r1_115.jpg|apple_red_3|\n",
      "|s3://fruitlabbucket/images/apple_red_3/r1_267.jpg|apple_red_3|\n",
      "|s3://fruitlabbucket/images/apple_red_3/r1_47.jpg |apple_red_3|\n",
      "|s3://fruitlabbucket/images/apple_red_3/r1_123.jpg|apple_red_3|\n",
      "+-------------------------------------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n",
      "Nombres d'images : 176"
     ]
    }
   ],
   "source": [
    "#===============================================================================================================\n",
    "# 3. CHARGEMENT DES IMAGES AU FORMAT BINAIRE\n",
    "#===============================================================================================================\n",
    "\n",
    "images = spark.read.format(\"binaryFile\") \\\n",
    "  .option(\"pathGlobFilter\", \"*.jpg\") \\\n",
    "  .option(\"recursiveFileLookup\", \"true\") \\\n",
    "  .load(\"s3://fruitlabbucket/images/*\")\n",
    "\n",
    "images = images.withColumn('label', element_at(split(images['path'], '/'),-2))\n",
    "print(images.printSchema())\n",
    "print(images.select('path','label').show(5,False))\n",
    "print(\"Nombres d'images :\", images.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f6b4605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt1/yarn/usercache/livy/appcache/application_1678693160646_0014/container_1678693160646_0014_01_000001/pyspark.zip/pyspark/sql/pandas/functions.py:398: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details."
     ]
    }
   ],
   "source": [
    "#===============================================================================================================\n",
    "# 4. PRÉPARATION DU MODÈLE MOBILENETV2 \n",
    "#===============================================================================================================\n",
    "\n",
    "model = MobileNetV2(weights='imagenet',\n",
    "                    include_top=True,\n",
    "                    input_shape=(224, 224, 3))\n",
    "\n",
    "new_model = Model(inputs=model.input, outputs=model.layers[-2].output)\n",
    "brodcast_weights = sc.broadcast(new_model.get_weights())\n",
    "\n",
    "def model_fn():\n",
    "    model = MobileNetV2(weights='imagenet',\n",
    "                        include_top=True,\n",
    "                        input_shape=(224, 224, 3))\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "    new_model = Model(inputs=model.input,\n",
    "                  outputs=model.layers[-2].output)\n",
    "    new_model.set_weights(brodcast_weights.value)\n",
    "    return new_model\n",
    "\n",
    "def preprocess(content):\n",
    "    img = Image.open(io.BytesIO(content)).resize([224, 224])\n",
    "    arr = img_to_array(img)\n",
    "    return preprocess_input(arr)\n",
    "\n",
    "def featurize_series(model, content_series):\n",
    "    input = np.stack(content_series.map(preprocess))\n",
    "    preds = model.predict(input)\n",
    "    output = [p.flatten() for p in preds]\n",
    "    return pd.Series(output)\n",
    "\n",
    "@pandas_udf('array<float>', PandasUDFType.SCALAR_ITER)\n",
    "def featurize_udf(content_series_iter):\n",
    "    model = model_fn()\n",
    "    for content_series in content_series_iter:\n",
    "        yield featurize_series(model, content_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a04e2df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "An error occurred while calling o257.parquet.\n",
      ": org.apache.spark.SparkException: Job aborted.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:651)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:279)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:193)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:104)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:114)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$7(SQLExecution.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:139)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:245)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:138)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:101)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:626)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:179)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:626)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:602)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:97)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:84)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:82)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:125)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:793)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 18.0 failed 4 times, most recent failure: Lost task 0.3 in stage 18.0 (TID 65) (ip-172-31-9-154.eu-west-3.compute.internal executor 1): java.lang.RuntimeException: Failed to run command: /usr/bin/virtualenv -p python3 --no-pip --system-site-packages virtualenv_application_1678693160646_0014_0\n",
      "\tat org.apache.spark.api.python.VirtualEnvFactory.execCommand(VirtualEnvFactory.scala:125)\n",
      "\tat org.apache.spark.api.python.VirtualEnvFactory.setupVirtualEnv(VirtualEnvFactory.scala:83)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.<init>(PythonWorkerFactory.scala:95)\n",
      "\tat org.apache.spark.SparkEnv.$anonfun$createPythonWorker$1(SparkEnv.scala:128)\n",
      "\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:128)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\n",
      "\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec.evaluate(ArrowEvalPythonExec.scala:88)\n",
      "\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:138)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1516)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2863)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2799)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2798)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2798)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1239)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1239)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1239)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3051)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2993)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2229)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:246)\n",
      "\t... 47 more\n",
      "Caused by: java.lang.RuntimeException: Failed to run command: /usr/bin/virtualenv -p python3 --no-pip --system-site-packages virtualenv_application_1678693160646_0014_0\n",
      "\tat org.apache.spark.api.python.VirtualEnvFactory.execCommand(VirtualEnvFactory.scala:125)\n",
      "\tat org.apache.spark.api.python.VirtualEnvFactory.setupVirtualEnv(VirtualEnvFactory.scala:83)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.<init>(PythonWorkerFactory.scala:95)\n",
      "\tat org.apache.spark.SparkEnv.$anonfun$createPythonWorker$1(SparkEnv.scala:128)\n",
      "\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:128)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\n",
      "\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec.evaluate(ArrowEvalPythonExec.scala:88)\n",
      "\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:138)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1516)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1678693160646_0014/container_1678693160646_0014_01_000001/pyspark.zip/pyspark/sql/readwriter.py\", line 1140, in parquet\n",
      "    self._jwrite.parquet(path)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1678693160646_0014/container_1678693160646_0014_01_000001/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1678693160646_0014/container_1678693160646_0014_01_000001/pyspark.zip/pyspark/sql/utils.py\", line 190, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1678693160646_0014/container_1678693160646_0014_01_000001/py4j-0.10.9.5-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o257.parquet.\n",
      ": org.apache.spark.SparkException: Job aborted.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:651)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:279)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:193)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:104)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:114)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$7(SQLExecution.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:224)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:139)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:245)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:138)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:101)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:626)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:179)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:626)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:602)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:97)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:84)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:82)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:125)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:793)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 18.0 failed 4 times, most recent failure: Lost task 0.3 in stage 18.0 (TID 65) (ip-172-31-9-154.eu-west-3.compute.internal executor 1): java.lang.RuntimeException: Failed to run command: /usr/bin/virtualenv -p python3 --no-pip --system-site-packages virtualenv_application_1678693160646_0014_0\n",
      "\tat org.apache.spark.api.python.VirtualEnvFactory.execCommand(VirtualEnvFactory.scala:125)\n",
      "\tat org.apache.spark.api.python.VirtualEnvFactory.setupVirtualEnv(VirtualEnvFactory.scala:83)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.<init>(PythonWorkerFactory.scala:95)\n",
      "\tat org.apache.spark.SparkEnv.$anonfun$createPythonWorker$1(SparkEnv.scala:128)\n",
      "\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:128)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\n",
      "\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec.evaluate(ArrowEvalPythonExec.scala:88)\n",
      "\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:138)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1516)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2863)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2799)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2798)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2798)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1239)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1239)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1239)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3051)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2993)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2229)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:246)\n",
      "\t... 47 more\n",
      "Caused by: java.lang.RuntimeException: Failed to run command: /usr/bin/virtualenv -p python3 --no-pip --system-site-packages virtualenv_application_1678693160646_0014_0\n",
      "\tat org.apache.spark.api.python.VirtualEnvFactory.execCommand(VirtualEnvFactory.scala:125)\n",
      "\tat org.apache.spark.api.python.VirtualEnvFactory.setupVirtualEnv(VirtualEnvFactory.scala:83)\n",
      "\tat org.apache.spark.api.python.PythonWorkerFactory.<init>(PythonWorkerFactory.scala:95)\n",
      "\tat org.apache.spark.SparkEnv.$anonfun$createPythonWorker$1(SparkEnv.scala:128)\n",
      "\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n",
      "\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:128)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\n",
      "\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec.evaluate(ArrowEvalPythonExec.scala:88)\n",
      "\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:131)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:855)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:138)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1516)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#===============================================================================================================\n",
    "# 5. EXTRACTION DE FEATURES\n",
    "#===============================================================================================================        \n",
    "\n",
    "features_df = images.repartition(20).select(col(\"path\"),\n",
    "                                            col(\"label\"),\n",
    "                                            featurize_udf(\"content\").alias(\"features\")\n",
    "                                           )\n",
    "\n",
    "features_df.write.mode(\"overwrite\").parquet('s3://fruitlabbucket/resultMobileNet/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775edd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#===============================================================================================================\n",
    "# 6. RÉDUCTION DIMENSIONNELLE\n",
    "#=============================================================================================================== \n",
    "\n",
    "data = spark.read.parquet('s3://fruitlabbucket/resultMobileNet/')\n",
    "to_dense_vector = udf(lambda arr: Vectors.dense(arr), VectorUDT()) \n",
    "data = data.withColumn(\"features_dense\", to_dense_vector(\"features\")) \n",
    "pca = PCA(k=2, inputCol=\"features_dense\", outputCol=\"pca_features\") \n",
    "model = pca.fit(data) \n",
    "transformed_data = model.transform(data) \n",
    "result = transformed_data.select(\"pca_features\") \n",
    "result.write.mode(\"overwrite\").parquet(\"s3://fruitlabbucket/resultPCA/\") \n",
    "\n",
    "def export_pca_features_to_csv(parquet_file_path, csv_file_path):\n",
    "    result_pca = pd.read_parquet(\"s3://fruitlabbucket/resultPCA/\", engine='pyarrow')\n",
    "    pca_features_numeric = []  \n",
    "    for row in result_pca['pca_features']:\n",
    "        pca_features_numeric.append([float(x) for x in row['values']])\n",
    "\n",
    "    pca_features_df = pd.DataFrame(pca_features_numeric, columns=['pca_feature_1', 'pca_feature_2'])\n",
    "    pca_features_df.to_csv(csv_file_path, index=False) \n",
    "\n",
    "export_pca_features_to_csv(\"s3://fruitlabbucket/resultPCA/\", \"s3://fruitlabbucket/resultPCA/resultPCA.csv\")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426b6b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#===============================================================================================================\n",
    "# 7. VISUALISATIONS DES RÉSULTATS\n",
    "#=============================================================================================================== \n",
    "\n",
    "df1 = pd.read_parquet(\"s3://fruitlabbucket/resultMobileNet/\", engine='pyarrow')\n",
    "df2 = pd.read_csv(\"s3://fruitlabbucket/resultPCA/resultPCA.csv\") \n",
    "df = pd.concat([df1, df2], axis=1)\n",
    "df.to_csv('s3://fruitlabbucket/matrice.csv', index=False) \n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
